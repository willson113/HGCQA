## Data processing and knowledge source preparation
For effective knowledge retrieval, it is essential to preprocess the knowledge sources that will support downstream tasks. In this study, we utilize the Chinese Wikipedia dump as our primary knowledge base. Specifically, we employ the complete XML dump released on August 20, 2024 (20240820 version), which contains the full corpus of Chinese Wikipedia entries spanning diverse domains including history, geography, and culture.

To enable efficient information extraction from this large-scale corpus, we implement rigorous preprocessing and standardization of the Wikipedia XML dump. This structured formatting facilitates subsequent knowledge retrieval and reasoning operations.
![image](https://github.com/user-attachments/assets/cb4e99dd-2fab-4a0e-9364-1a01cc6a4b88)

## Our vector database
![1745160673325](https://github.com/user-attachments/assets/72d16cf1-849e-42b9-8968-ea351f721c7b)



## Framework Diagram for Text-Based Keyword Retrieval
![1744726091068](https://github.com/user-attachments/assets/1c9987e0-50e8-414e-970e-30095453f9f2)


## Framework Diagram for Semantic search
![1744726058285](https://github.com/user-attachments/assets/d751940e-4570-42c8-9d9c-70413432f3d1)



## Framework Diagram for Hybrid retrieval
![1744726117708](https://github.com/user-attachments/assets/ca8eb95c-60d4-4fd1-8b68-356bca808c0c)
