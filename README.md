# HGCQA: A Benchmark for Complex Question Answering in Chinese History and Geography

**HGCQA (Chinese History and Geography Complex Question Answering)** the first large-scale Chinese CQA benchmark specifically designed for history and geography domains, comprising 10,103 expert annotated question-answer pairs with multi-stage quality validation.
---Research Challenge:
Large Language Models (LLMs) exhibit notable limitations in handling complex question answering (CQA) tasks within the domain of Chinese history and geography, primarily due to knowledge gaps and inadequate reasoning capabilities.

---Key Issues:
Domain-Specific Knowledge Deficits
LLMs lack comprehensive coverage of nuanced historical events, cultural contexts, and geographical specifics unique to China.

---Reasoning Shortcomings
Struggles with multi-hop reasoning (e.g., linking dynastic timelines to regional changes).
Weak performance on comparative (e.g., "Compare the administrative systems of Tang and Ming dynasties") and temporal reasoning tasks.

---Language and Context Barriers
Classical Chinese terms, archaic place names, and culturally embedded metaphors further complicate comprehension.
![NLPCC1](https://github.com/user-attachments/assets/2ebc7de3-5500-471b-89ca-8c0b879db8e3)

---
## ğŸ“Œ Introduction
Most existing CQA datasets are designed for **open-domain English questions**. In contrast, real-world applications often focus on **vertical domains** in **non-English languages**, especially Chinese.
To address this gap, we introduce **HGCQA**, a benchmark dataset of **10,103 Chinese complex questions**, annotated with:
- Gold-standard answers
- One of 8 complexity types:
  - Bool
  - Comparison
  - Count
  - Difference
  - Intersection
  - Multi-hop
  - Ordinal
  - General

HGCQA is the first benchmark focusing on **Chinese CQA** in history and geography.

---



## Dataset Construction Process
![n30](https://github.com/user-attachments/assets/ffe13017-3c95-4fab-892c-086772bc755a)


## Question Generation Method
![å›¾3-3](https://github.com/user-attachments/assets/fc80f25c-0668-446c-b6b7-219e805520ab)


## ğŸ§  Dataset Overview
- ğŸ§¾ Total questions: `10,103`
- ğŸ“š Source: Chinese Wikipedia (filtered for historical and geographical content)
- ğŸ·ï¸ Format: JSON
  ```json
  {
    "question": "èµµå…è®©çš„ç¬¬åä¸‰å­åœ¨ä½æœŸé—´çš„å¹´å·æ˜¯ä»€ä¹ˆï¼Ÿ",
    "answer": "æ²»å¹³",
    "type": "Multi-hop"
  },
  {
    "question": "ä¸­å›½è¥¿éƒ¨ç¬¬å››å¤§åŸå¸‚æ˜¯ï¼Ÿ",
    "answer": "æ˜†æ˜å¸‚",
    "type":"Ordinal"
  },
  {
    "question": "å°åˆ·æœ¯æ˜¯å¦æ˜¯å—å®‹å¼€å§‹å‡ºç°çš„ï¼Ÿ",
    "answer": "å¦",
    "type":"Bool"
  },
  {
    "question": "å…ƒæœå…±ç»å†äº†å¤šå°‘ä½çš‡å¸ï¼Ÿ",
    "answer": "11ä½",
    "type": "Count"
  },
  {
    "question": "ä¸¤æ¹–å¹³åŸçš„è¥¿éƒ¨è¾¹ç•Œæ˜¯ä»€ä¹ˆå±±è„‰ï¼Ÿ",
    "answer": "å¤§åˆ«å±±",
    "type":"General"
  },
  {
   "question": "å…ƒæœå“ªä½è’™å¤ç‹å…¬å¤§è‡£æ˜¯è™”è¯šçš„ä¼Šæ–¯å…°æ•™å¾’ï¼Œå¹¶ä¸”æ›¾ç»å¾ˆæœ‰æœºä¼šç»§æ‰¿ä¸ºçš‡å¸ï¼Ÿ",
   "answer": "å®‰è¥¿ç‹é˜¿éš¾ç­”",
   "type":"Intersection"
  },
  {
   "question": "é‡‘å¤ªå®—å’Œé‡‘ç†™å®—ï¼Œè°çš„åœ¨ä½æ—¶é—´æ›´é•¿?",
   "answer": "é‡‘ç†™å®—",
   "type":"Comparison"
  },
  {
   "question": "é•¿æ±Ÿä¸‰å¤§çŸ¶ä¸åŒ…æ‹¬ä»¥ä¸‹å“ªä¸ªï¼šç‡•å­çŸ¶ã€é‡‡çŸ³çŸ¶ã€å¹¿é™µçŸ¶ï¼Ÿ",
    "answer": "å¹¿é™µçŸ¶",
    "type":"Difference"
  }
## Evaluation Metrics and Methods
We adopt exact match (EM) as the
evaluation metric. However, since LLMs often generate answers in descriptive forms while the dataset provides concise answer phrases, direct EM matching becomes challenging. To address this, we employ LLM-based evaluation to better align with model outputs. Based on this, we report both overall and per-type accuracy, providing a comprehensive assessment of LLM performance.
![å›¾1-3](https://github.com/user-attachments/assets/0ffcf7da-da6f-4569-8de1-13a8ad790f88)

  
## ğŸš€ Baselines and Experimental Results
We evaluate 7 large language models across 3 retrieval methods and 3 reasoning strategies:
Retrieval Methods: BM25, Dense Retrieval (bge-m3), Hybrid Retrievalï¼ŒThey are implemented based on the LangChain framework.
#### General framework with RAG for augmented experiments
![n31](https://github.com/user-attachments/assets/6d783428-466f-46b8-a24e-ad1c194aefce)


## Reasoning Prompts: COT, ThoT, ReAct

## COT Prompt Structure
![å›¾4-4](https://github.com/user-attachments/assets/65da3ec4-b21b-4aac-8e82-32dd7bcbd91f)
ThoT Prompt Structure
![å›¾4-5](https://github.com/user-attachments/assets/197ae2b2-19fe-425b-9f4f-a76594dbdb30)
ReAct Prompt Structure
![å›¾4-6](https://github.com/user-attachments/assets/40c149fd-5b52-4679-8375-c06559480eb1)

LLMs for experiment,The evaluated models are hosted on Baidu Intelligent Cloud Platform and Zhipu AI, with inference performed through their respective APIs.
Llama2-7B
Chatglm2-6B
Llama2-13B
Llama2-70B
Glm3-turbo
ERNIE-3.5
Glm-4


ğŸ” Best-performing setup:
Model: GLM-4
Retrieval: Hybrid Retrieval
Prompt: ReAct
Best typeï¼šDifference 94.60%
Worst type: Ordinal 76.80%
Accuracy: 89.18%

Worst-performing setup 
Model:Llama2-7B
Retrieval:  BM25
Prompt: COT
Best typeï¼šGeneral 53.46%
Worst type: Ordinal 32.15%
Accuracy: 36.99%
For full benchmark results, see the paper and results/ directory.
